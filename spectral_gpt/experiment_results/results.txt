â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ ğŸ”¬ WAVE-NATIVE GPT ABLATION SUITE               â”‚
â”‚                                                 â”‚
â”‚ Model: Small (GPT-2 Compatible)                 â”‚
â”‚ Dataset: fineweb                                â”‚
â”‚ Experiments: standard_transformer, full_physics â”‚
â”‚ Steps per experiment: 15000                     â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
ğŸ“¥ Loading FineWeb-Edu (sample-10BT) via Tiktoken...
   Target: 500,000,000 tokens
README.md: 26.4kB [00:00, 57.8MB/s]
Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2410/2410 [00:00<00:00, 34808.70it/s]
Streaming & Tokenizing... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:00:0000:0100:10
âœ… Loaded exactly 500,000,000 tokens.
ğŸ“Š Tokens: Train 450,000,000 | Val 50,000,000

============================================================
ğŸ“Š Parameters: 52,892,160 (52.89M)
ğŸ”€ Using DataParallel on 2 GPUs
          ğŸ“š Dataset Statistics           
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value               â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ Total Tokens     â”‚ 500,000,000         â”‚
â”‚ Train Split      â”‚ 450,000,000 (90.0%) â”‚
â”‚ Validation Split â”‚ 50,000,000 (10.0%)  â”‚
â”‚ Tokenizer        â”‚ TikToken (GPT-2)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          ğŸ§  Model Parameter Breakdown          
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Component          â”ƒ Parameters â”ƒ % of Total â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ Embeddings         â”‚ 19,396,992 â”‚      36.7% â”‚
â”‚ Attention          â”‚  4,730,880 â”‚       8.9% â”‚
â”‚ MLP                â”‚  9,452,544 â”‚      17.9% â”‚
â”‚ Other (Norms/Head) â”‚ 19,311,744 â”‚      36.5% â”‚
â”‚ TOTAL              â”‚ 52,892,160 â”‚     100.0% â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
ğŸ“Š Ratio: 9.45 Tokens per Parameter (Target > 7.4)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Standard Transformer (GPT-2)                                                 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
ğŸ“Š Parameters: 52,892,160 (52.89M)
ğŸ”„ Steps: 15000
âš™ï¸  Optimizer: AdamW
ğŸ“‰ Loss: Cross-Entropy
ğŸ“ˆ LR: 6e-04
/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70:    0% â€¢ Loss: 0.0000 0:00:01
UserWarning: Was asked to gather along dimension 0, but all input tensors were 
scalars; will instead unsqueeze and return a vector.
  warnings.warn(
Step   250 | Train(CE): 7.0259 | Val: 7.0833 | AvgTrain: 8.3495 | R: 0.500
   New best: 7.0833 (Saved)0m   2% â€¢ Loss: 7.0259 0:01:03
Step   500 | Train(CE): 6.5191 | Val: 6.5897 | AvgTrain: 6.7990 | R: 0.500
   New best: 6.5897 (Saved)â”â”â”â”â”â”â”   3% â€¢ Loss: 6.5191 0:02:07
Step   750 | Train(CE): 6.5148 | Val: 6.3787 | AvgTrain: 6.5098 | R: 0.500
   New best: 6.3787 (Saved)â”â”â”â”â”â”â”   5% â€¢ Loss: 6.5148 0:03:11
Step  1000 | Train(CE): 6.1513 | Val: 6.2138 | AvgTrain: 6.3093 | R: 0.500
   New best: 6.2138 (Saved)â”â”â”â”â”â”â”   7% â€¢ Loss: 6.1513 0:04:16
Step  1250 | Train(CE): 6.0445 | Val: 6.0489 | AvgTrain: 6.1542 | R: 0.500
   New best: 6.0489 (Saved)â”â”â”â”â”â”â”   8% â€¢ Loss: 6.0445 0:05:21
Step  1500 | Train(CE): 5.9265 | Val: 5.8941 | AvgTrain: 6.0012 | R: 0.500
   New best: 5.8941 (Saved)â”â”â”â”â”â”â”  10% â€¢ Loss: 5.9265 0:06:26
Step  1750 | Train(CE): 6.0348 | Val: 5.7979 | AvgTrain: 5.8864 | R: 0.500
   New best: 5.7979 (Saved)â”â”â”â”â”â”â”  12% â€¢ Loss: 6.0348 0:07:31
Step  2000 | Train(CE): 5.8391 | Val: 5.6770 | AvgTrain: 5.7816 | R: 0.500
   New best: 5.6770 (Saved)â”â”â”â”â”â”â”  13% â€¢ Loss: 5.8391 0:08:36
Step  2250 | Train(CE): 5.7084 | Val: 5.6168 | AvgTrain: 5.6791 | R: 0.500
   New best: 5.6168 (Saved)â”â”â”â”â”â”â”  15% â€¢ Loss: 5.7084 0:09:41
Step  2500 | Train(CE): 5.6050 | Val: 5.4793 | AvgTrain: 5.6067 | R: 0.500
   New best: 5.4793 (Saved)â”â”â”â”â”â”â”  17% â€¢ Loss: 5.6050 0:10:46
Step  2750 | Train(CE): 5.1174 | Val: 5.4798 | AvgTrain: 5.5340 | R: 0.500
   Patience: 1/80mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  18% â€¢ Loss: 5.1174 0:11:51
Step  3000 | Train(CE): 5.2369 | Val: 5.3670 | AvgTrain: 5.4608 | R: 0.500
   New best: 5.3670 (Saved)â”â”â”â”â”â”â”  20% â€¢ Loss: 5.2369 0:12:56
Step  3250 | Train(CE): 5.2816 | Val: 5.3366 | AvgTrain: 5.3963 | R: 0.500
   New best: 5.3366 (Saved)â”â”â”â”â”â”â”  22% â€¢ Loss: 5.2816 0:14:01
Step  3500 | Train(CE): 5.3466 | Val: 5.3028 | AvgTrain: 5.3377 | R: 0.500
   New best: 5.3028 (Saved)â”â”â”â”â”â”â”  23% â€¢ Loss: 5.3466 0:15:06
Step  3750 | Train(CE): 5.2372 | Val: 5.2334 | AvgTrain: 5.3110 | R: 0.500
   New best: 5.2334 (Saved)â”â”â”â”â”â”â”  25% â€¢ Loss: 5.2372 0:16:11
Step  4000 | Train(CE): 5.1283 | Val: 5.1998 | AvgTrain: 5.2604 | R: 0.500
   New best: 5.1998 (Saved)â”â”â”â”â”â”â”  27% â€¢ Loss: 5.1283 0:17:16
Step  4250 | Train(CE): 5.2175 | Val: 5.1597 | AvgTrain: 5.1886 | R: 0.500
   New best: 5.1597 (Saved)â”â”â”â”â”â”â”  28% â€¢ Loss: 5.2175 0:18:21
Step  4500 | Train(CE): 5.1382 | Val: 5.0958 | AvgTrain: 5.1677 | R: 0.500
   New best: 5.0958 (Saved)â”â”â”â”â”â”â”  30% â€¢ Loss: 5.1382 0:19:26
Step  4750 | Train(CE): 5.0867 | Val: 5.0458 | AvgTrain: 5.1140 | R: 0.500
   New best: 5.0458 (Saved)â”â”â”â”â”â”â”  32% â€¢ Loss: 5.0867 0:20:31
Step  5000 | Train(CE): 5.1364 | Val: 4.9868 | AvgTrain: 5.0969 | R: 0.500
   New best: 4.9868 (Saved)â”â”â”â”â”â”â”  33% â€¢ Loss: 5.1364 0:21:36
Step  5250 | Train(CE): 5.0765 | Val: 5.0112 | AvgTrain: 5.0581 | R: 0.500
   Patience: 1/8â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  35% â€¢ Loss: 5.0765 0:22:40
Step  5500 | Train(CE): 5.1330 | Val: 4.9566 | AvgTrain: 5.0059 | R: 0.500
   New best: 4.9566 (Saved)â”â”â”â”â”â”â”  37% â€¢ Loss: 5.1330 0:23:45
Step  5750 | Train(CE): 5.0285 | Val: 4.9282 | AvgTrain: 4.9966 | R: 0.500
   New best: 4.9282 (Saved)â”â”â”â”â”â”â”  38% â€¢ Loss: 5.0285 0:24:51
Step  6000 | Train(CE): 4.8919 | Val: 4.9163 | AvgTrain: 4.9435 | R: 0.500
   New best: 4.9163 (Saved)â”â”â”â”â”â”â”  40% â€¢ Loss: 4.8919 0:25:56
Step  6250 | Train(CE): 4.8839 | Val: 4.8209 | AvgTrain: 4.9302 | R: 0.500
   New best: 4.8209 (Saved)â”â”â”â”â”â”â”  42% â€¢ Loss: 4.8839 0:27:01
Step  6500 | Train(CE): 4.7993 | Val: 4.8593 | AvgTrain: 4.8973 | R: 0.500
   Patience: 1/80mâ•ºâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  43% â€¢ Loss: 4.7993 0:28:06
Step  6750 | Train(CE): 4.7793 | Val: 4.8212 | AvgTrain: 4.8764 | R: 0.500
   Patience: 2/890mâ•ºâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  45% â€¢ Loss: 4.7793 0:29:10
Step  7000 | Train(CE): 4.7806 | Val: 4.7565 | AvgTrain: 4.8451 | R: 0.500
   New best: 4.7565 (Saved)â”â”â”â”â”â”â”  47% â€¢ Loss: 4.7806 0:30:16
Step  7250 | Train(CE): 4.7286 | Val: 4.7912 | AvgTrain: 4.8102 | R: 0.500
   Patience: 1/8[90mâ•ºâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  48% â€¢ Loss: 4.7286 0:31:21
Step  7500 | Train(CE): 4.8588 | Val: 4.7790 | AvgTrain: 4.7951 | R: 0.500
   Patience: 2/8â•ºâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  50% â€¢ Loss: 4.8588 0:32:25
Step  7750 | Train(CE): 4.7725 | Val: 4.7856 | AvgTrain: 4.7736 | R: 0.500
   Patience: 3/8â•¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  52% â€¢ Loss: 4.7725 0:33:31
Step  8000 | Train(CE): 4.7634 | Val: 4.7325 | AvgTrain: 4.7478 | R: 0.500
   New best: 4.7325 (Saved)â”â”â”â”â”â”â”  53% â€¢ Loss: 4.7634 0:34:36
Step  8250 | Train(CE): 4.6497 | Val: 4.7182 | AvgTrain: 4.7392 | R: 0.500
   New best: 4.7182 (Saved)â”â”â”â”â”â”â”  55% â€¢ Loss: 4.6497 0:35:41
Step  8500 | Train(CE): 4.6950 | Val: 4.6496 | AvgTrain: 4.7126 | R: 0.500
   New best: 4.6496 (Saved)â”â”â”â”â”â”â”  57% â€¢ Loss: 4.6950 0:36:46
Step  8750 | Train(CE): 4.7980 | Val: 4.6513 | AvgTrain: 4.6925 | R: 0.500
   Patience: 1/8[0mâ•ºâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  58% â€¢ Loss: 4.7980 0:37:51
Step  9000 | Train(CE): 4.5088 | Val: 4.6635 | AvgTrain: 4.6754 | R: 0.500
   Patience: 2/8â•ºâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  60% â€¢ Loss: 4.5088 0:38:56
Step  9250 | Train(CE): 4.5221 | Val: 4.5962 | AvgTrain: 4.6753 | R: 0.500
   New best: 4.5962 (Saved)â”â”â”â”â”â”â”  62% â€¢ Loss: 4.5221 0:40:01
Step  9500 | Train(CE): 4.6520 | Val: 4.5843 | AvgTrain: 4.6376 | R: 0.500
   New best: 4.5843 (Saved)â”â”â”â”â”â”â”  63% â€¢ Loss: 4.6520 0:41:06
Step  9750 | Train(CE): 4.7869 | Val: 4.5780 | AvgTrain: 4.6147 | R: 0.500
   New best: 4.5780 (Saved)â”â”â”â”â”â”â”  65% â€¢ Loss: 4.7869 0:42:11
Step 10000 | Train(CE): 4.5243 | Val: 4.5601 | AvgTrain: 4.6082 | R: 0.500
   New best: 4.5601 (Saved)â”â”â”â”â”â”â”  67% â€¢ Loss: 4.5243 0:43:17
Step 10250 | Train(CE): 4.6660 | Val: 4.6133 | AvgTrain: 4.6103 | R: 0.500
   Patience: 1/8â”â”â”â•ºâ”â”â”â”â”â”â”â”â”â”â”â”  68% â€¢ Loss: 4.6660 0:44:22
Step 10500 | Train(CE): 4.5115 | Val: 4.5460 | AvgTrain: 4.5882 | R: 0.500
   New best: 4.5460 (Saved)â”â”â”â”â”â”â”  70% â€¢ Loss: 4.5115 0:45:27
Step 10750 | Train(CE): 4.5079 | Val: 4.5691 | AvgTrain: 4.5846 | R: 0.500
   Patience: 1/8â”â”â”â”â•¸â”â”â”â”â”â”â”â”â”â”â”  72% â€¢ Loss: 4.5079 0:46:32
Step 11000 | Train(CE): 4.4894 | Val: 4.5038 | AvgTrain: 4.5732 | R: 0.500
   New best: 4.5038 (Saved)â”â”â”â”â”â”â”  73% â€¢ Loss: 4.4894 0:47:37
Step 11250 | Train(CE): 4.4280 | Val: 4.5011 | AvgTrain: 4.5615 | R: 0.500
   Patience: 1/8â”â”â”â”â”â”â•ºâ”â”â”â”â”â”â”â”â”  75% â€¢ Loss: 4.4280 0:48:42
Step 11500 | Train(CE): 4.6503 | Val: 4.5092 | AvgTrain: 4.5614 | R: 0.500
   Patience: 2/8â”â”â”â”â”â”â•¸â”â”â”â”â”â”â”â”â”  77% â€¢ Loss: 4.6503 0:49:47
Step 11750 | Train(CE): 4.5941 | Val: 4.5134 | AvgTrain: 4.5478 | R: 0.500
   Patience: 3/8â”â”â”â”â”â”â”â•ºâ”â”â”â”â”â”â”â”  78% â€¢ Loss: 4.5941 0:50:52
Step 12000 | Train(CE): 4.5213 | Val: 4.5187 | AvgTrain: 4.5515 | R: 0.500
   Patience: 4/8â”â”â”â”â”â”â”â”â•ºâ”â”â”â”â”â”â”  80% â€¢ Loss: 4.5213 0:51:57
Step 12250 | Train(CE): 4.4920 | Val: 4.5016 | AvgTrain: 4.5358 | R: 0.500
   Patience: 5/8â”â”â”â”â”â”â”â”â•¸â”â”â”â”â”â”â”  82% â€¢ Loss: 4.4920 0:53:02
Step 12500 | Train(CE): 4.4138 | Val: 4.4822 | AvgTrain: 4.5205 | R: 0.500
   New best: 4.4822 (Saved)mâ”â”â”â”â”â”  83% â€¢ Loss: 4.4138 0:54:08
Step 12750 | Train(CE): 4.4052 | Val: 4.4894 | AvgTrain: 4.5263 | R: 0.500
   Patience: 1/8â”â”â”â”â”â”â”â”â”â”â•ºâ”â”â”â”â”  85% â€¢ Loss: 4.4052 0:55:13
Step 13000 | Train(CE): 4.6315 | Val: 4.4840 | AvgTrain: 4.5193 | R: 0.500
   Patience: 2/8â”â”â”â”â”â”â”â”â”â”â•¸â”â”â”â”â”  87% â€¢ Loss: 4.6315 0:56:18
Step 13250 | Train(CE): 4.2857 | Val: 4.4783 | AvgTrain: 4.4974 | R: 0.500
   Patience: 3/8â”â”â”â”â”â”â”â”â”â”â”â•ºâ”â”â”â”  88% â€¢ Loss: 4.2857 0:57:23
Step 13500 | Train(CE): 4.4044 | Val: 4.4382 | AvgTrain: 4.5134 | R: 0.500
   New best: 4.4382 (Saved)[90mâ”â”â”  90% â€¢ Loss: 4.4044 0:58:28
Step 13750 | Train(CE): 4.6931 | Val: 4.5201 | AvgTrain: 4.5170 | R: 0.500
   Patience: 1/8â”â”â”â”â”â”â”â”â”â”â”â”â•¸â”â”â”  92% â€¢ Loss: 4.6931 0:59:33
Step 14000 | Train(CE): 4.5549 | Val: 4.4628 | AvgTrain: 4.4991 | R: 0.500
   Patience: 2/8â”â”â”â”â”â”â”â”â”â”â”â”â”â•ºâ”â”  93% â€¢ Loss: 4.5549 1:00:38
Step 14250 | Train(CE): 4.6113 | Val: 4.5096 | AvgTrain: 4.5061 | R: 0.500
   Patience: 3/8â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•ºâ”  95% â€¢ Loss: 4.6113 1:01:43
Step 14500 | Train(CE): 4.6553 | Val: 4.4791 | AvgTrain: 4.5047 | R: 0.500
   Patience: 4/8â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸â”  97% â€¢ Loss: 4.6553 1:02:48
Step 14750 | Train(CE): 4.4297 | Val: 4.4880 | AvgTrain: 4.5011 | R: 0.500
   Patience: 5/8â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•º  98% â€¢ Loss: 4.4297 1:03:54
Step 15000 | Train(CE): 4.4885 | Val: 4.5114 | AvgTrain: 4.4972 | R: 0.500
   Patience: 6/8â”â”â”â”â”â”â” 100% â€¢ Loss: 4.4885 1:04:56
  Training â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% â€¢ Loss: 4.4885 1:04:56
â™»ï¸  Restoring best model (Val: 4.4382)

ğŸ¨ Generating sample (Visual Check)...
ğŸ“ Generation Check:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Standard Transformer (GPT-2) Output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ The theory of quantum mechanics suggests that a large number of computers    â”‚
â”‚ are capable of both the same size and speed of motion. In this development,  â”‚
â”‚ the structure of quantum mechanics is a wonderful way to quantum physics for â”‚
â”‚ autonomous ideas, and all other quantum mechanics are required to compute    â”‚
â”‚ angles, but they are not suited to the same magnitude. In this production,   â”‚
â”‚ theory has been divided into three categories: the dimensions of a particle, â”‚
â”‚ the constraints of a cell and the types of instruments in which a physical   â”‚
â”‚ internal computation is created using quantum mechanics. In                  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ ğŸ”¬ WAVE-NATIVE GPT ABLATION SUITE   â”‚
â”‚                                     â”‚
â”‚ Model: Small (GPT-2 Compatible)     â”‚
â”‚ Dataset: fineweb                    â”‚
â”‚ Experiments: full_physics, rgd_only â”‚
â”‚ Steps per experiment: 15000         â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
ğŸ“¥ Loading FineWeb-Edu (sample-10BT) via Tiktoken...
   Target: 500,000,000 tokens
README.md: 26.4kB [00:00, 54.9MB/s]
Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2410/2410 [00:00<00:00, 32696.98it/s]
Streaming & Tokenizing... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:00:0000:0100:11
âœ… Loaded exactly 500,000,000 tokens.
ğŸ“Š Tokens: Train 450,000,000 | Val 50,000,000

============================================================
ğŸ“Š Parameters: 67,473,706 (67.47M)
ğŸ”€ Using DataParallel on 2 GPUs
          ğŸ“š Dataset Statistics           
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value               â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ Total Tokens     â”‚ 500,000,000         â”‚
â”‚ Train Split      â”‚ 450,000,000 (90.0%) â”‚
â”‚ Validation Split â”‚ 50,000,000 (10.0%)  â”‚
â”‚ Tokenizer        â”‚ TikToken (GPT-2)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          ğŸ§  Model Parameter Breakdown          
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Component          â”ƒ Parameters â”ƒ % of Total â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ Embeddings         â”‚ 33,921,361 â”‚      50.3% â”‚
â”‚ Attention          â”‚  4,737,032 â”‚       7.0% â”‚
â”‚ MLP                â”‚  9,452,544 â”‚      14.0% â”‚
â”‚ Other (Norms/Head) â”‚ 19,362,769 â”‚      28.7% â”‚
â”‚ TOTAL              â”‚ 67,473,706 â”‚     100.0% â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
ğŸ“Š Ratio: 7.41 Tokens per Parameter (Target > 7.4)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Full Physics (RGD + QFE)                                                     â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
ğŸ“Š Parameters: 67,473,706 (67.47M)
ğŸ”„ Steps: 15000
âš¡ Optimizer: RGD (strength=0.3)
ğŸŒŒ Loss: QFE (Î»=0.1)
ğŸ“ˆ LR: 1e-03
/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70:    0% â€¢ Loss: 0.0000 0:00:01
UserWarning: Was asked to gather along dimension 0, but all input tensors were 
scalars; will instead unsqueeze and return a vector.
  warnings.warn(
Step   250 | Train(CE): 6.8202 | Val: 6.7307 | AvgTrain: 7.9331 | R: 0.505
   New best: 6.7307 (Saved)0m   2% â€¢ Loss: 6.8202 0:04:20
Step   500 | Train(CE): 6.3997 | Val: 6.3905 | AvgTrain: 6.5664 | R: 0.493
   New best: 6.3905 (Saved)â”â”â”â”â”â”â”   3% â€¢ Loss: 6.3997 0:08:43
Step   750 | Train(CE): 6.2360 | Val: 6.2000 | AvgTrain: 6.2989 | R: 0.496
   New best: 6.2000 (Saved)â”â”â”â”â”â”â”   5% â€¢ Loss: 6.2360 0:13:07
Step  1000 | Train(CE): 6.0337 | Val: 6.0366 | AvgTrain: 6.1104 | R: 0.504
   New best: 6.0366 (Saved)â”â”â”â”â”â”â”   7% â€¢ Loss: 6.0337 0:17:30
Step  1250 | Train(CE): 5.8077 | Val: 5.9553 | AvgTrain: 5.9885 | R: 0.510
   New best: 5.9553 (Saved)â”â”â”â”â”â”â”   8% â€¢ Loss: 5.8077 0:21:54
Step  1500 | Train(CE): 5.8832 | Val: 5.8654 | AvgTrain: 5.8879 | R: 0.517
   New best: 5.8654 (Saved)â”â”â”â”â”â”â”  10% â€¢ Loss: 5.8832 0:26:18
Step  1750 | Train(CE): 5.9402 | Val: 5.8076 | AvgTrain: 5.8193 | R: 0.524
   New best: 5.8076 (Saved)â”â”â”â”â”â”â”  12% â€¢ Loss: 5.9402 0:30:41
Step  2000 | Train(CE): 5.7769 | Val: 5.7531 | AvgTrain: 5.7527 | R: 0.530
   New best: 5.7531 (Saved)â”â”â”â”â”â”â”  13% â€¢ Loss: 5.7769 0:35:05
Step  2250 | Train(CE): 5.7194 | Val: 5.6685 | AvgTrain: 5.6946 | R: 0.537
   New best: 5.6685 (Saved)â”â”â”â”â”â”â”  15% â€¢ Loss: 5.7194 0:39:29
Step  2500 | Train(CE): 5.5482 | Val: 5.6707 | AvgTrain: 5.6511 | R: 0.543
   Patience: 1/8mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  17% â€¢ Loss: 5.5482 0:43:53
Step  2750 | Train(CE): 5.5133 | Val: 5.6239 | AvgTrain: 5.5976 | R: 0.551
   New best: 5.6239 (Saved)â”â”â”â”â”â”â”  18% â€¢ Loss: 5.5133 0:48:17
Step  3000 | Train(CE): 5.4477 | Val: 5.5741 | AvgTrain: 5.5499 | R: 0.558
   New best: 5.5741 (Saved)â”â”â”â”â”â”â”  20% â€¢ Loss: 5.4477 0:52:41
Step  3250 | Train(CE): 5.5923 | Val: 5.5255 | AvgTrain: 5.5184 | R: 0.565
   New best: 5.5255 (Saved)â”â”â”â”â”â”â”  22% â€¢ Loss: 5.5923 0:57:05
Step  3500 | Train(CE): 5.6239 | Val: 5.5390 | AvgTrain: 5.4925 | R: 0.572
   Patience: 1/8[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  23% â€¢ Loss: 5.6239 1:01:28
Step  3750 | Train(CE): 5.5523 | Val: 5.4092 | AvgTrain: 5.4399 | R: 0.578
   New best: 5.4092 (Saved)â”â”â”â”â”â”â”  25% â€¢ Loss: 5.5523 1:05:53
Step  4000 | Train(CE): 5.4782 | Val: 5.4537 | AvgTrain: 5.4235 | R: 0.586
   Patience: 1/8â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  27% â€¢ Loss: 5.4782 1:10:16
Step  4250 | Train(CE): 5.4098 | Val: 5.3569 | AvgTrain: 5.3933 | R: 0.593
   New best: 5.3569 (Saved)â”â”â”â”â”â”â”  28% â€¢ Loss: 5.4098 1:14:40
Step  4500 | Train(CE): 5.2844 | Val: 5.3893 | AvgTrain: 5.3534 | R: 0.600
   Patience: 1/80mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  30% â€¢ Loss: 5.2844 1:19:04
Step  4750 | Train(CE): 5.1987 | Val: 5.3141 | AvgTrain: 5.3289 | R: 0.607
   New best: 5.3141 (Saved)â”â”â”â”â”â”â”  32% â€¢ Loss: 5.1987 1:23:28
Step  5000 | Train(CE): 5.0546 | Val: 5.2725 | AvgTrain: 5.2845 | R: 0.614
   New best: 5.2725 (Saved)â”â”â”â”â”â”â”  33% â€¢ Loss: 5.0546 1:27:52
Step  5250 | Train(CE): 5.2150 | Val: 5.2445 | AvgTrain: 5.2278 | R: 0.622
   New best: 5.2445 (Saved)â”â”â”â”â”â”â”  35% â€¢ Loss: 5.2150 1:32:16
Step  5500 | Train(CE): 5.2094 | Val: 5.2245 | AvgTrain: 5.1881 | R: 0.629
   New best: 5.2245 (Saved)â”â”â”â”â”â”â”  37% â€¢ Loss: 5.2094 1:36:40
Step  5750 | Train(CE): 5.3009 | Val: 5.1247 | AvgTrain: 5.1266 | R: 0.637
   New best: 5.1247 (Saved)â”â”â”â”â”â”â”  38% â€¢ Loss: 5.3009 1:41:04
Step  6000 | Train(CE): 4.8357 | Val: 5.0666 | AvgTrain: 5.0697 | R: 0.644
   New best: 5.0666 (Saved)â”â”â”â”â”â”â”  40% â€¢ Loss: 4.8357 1:45:28
Step  6250 | Train(CE): 5.1557 | Val: 5.0586 | AvgTrain: 5.0276 | R: 0.651
   New best: 5.0586 (Saved)â”â”â”â”â”â”â”  42% â€¢ Loss: 5.1557 1:49:51
Step  6500 | Train(CE): 4.9923 | Val: 5.0058 | AvgTrain: 4.9837 | R: 0.658
   New best: 5.0058 (Saved)â”â”â”â”â”â”â”  43% â€¢ Loss: 4.9923 1:54:15
Step  6750 | Train(CE): 4.8085 | Val: 5.0055 | AvgTrain: 4.9715 | R: 0.666
   Patience: 1/890mâ•ºâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  45% â€¢ Loss: 4.8085 1:58:39
Step  7000 | Train(CE): 4.8495 | Val: 4.8879 | AvgTrain: 4.9280 | R: 0.673
   New best: 4.8879 (Saved)â”â”â”â”â”â”â”  47% â€¢ Loss: 4.8495 2:03:03
Step  7250 | Train(CE): 4.5991 | Val: 4.8801 | AvgTrain: 4.9001 | R: 0.681
   New best: 4.8801 (Saved)â”â”â”â”â”â”â”  48% â€¢ Loss: 4.5991 2:07:27
Step  7500 | Train(CE): 4.8007 | Val: 4.9213 | AvgTrain: 4.8623 | R: 0.688
   Patience: 1/8â•ºâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  50% â€¢ Loss: 4.8007 2:11:51
Step  7750 | Train(CE): 4.7664 | Val: 4.8789 | AvgTrain: 4.8460 | R: 0.695
   Patience: 2/8â•¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  52% â€¢ Loss: 4.7664 2:16:15
Step  8000 | Train(CE): 4.8185 | Val: 4.8583 | AvgTrain: 4.8057 | R: 0.702
   New best: 4.8583 (Saved)â”â”â”â”â”â”â”  53% â€¢ Loss: 4.8185 2:20:39
Step  8250 | Train(CE): 4.5753 | Val: 4.8245 | AvgTrain: 4.7905 | R: 0.710
   New best: 4.8245 (Saved)â”â”â”â”â”â”â”  55% â€¢ Loss: 4.5753 2:25:03
Step  8500 | Train(CE): 4.9269 | Val: 4.8030 | AvgTrain: 4.7729 | R: 0.717
   New best: 4.8030 (Saved)â”â”â”â”â”â”â”  57% â€¢ Loss: 4.9269 2:29:27
Step  8750 | Train(CE): 4.6614 | Val: 4.7709 | AvgTrain: 4.7526 | R: 0.724
   New best: 4.7709 (Saved)â”â”â”â”â”â”â”  58% â€¢ Loss: 4.6614 2:33:51
Step  9000 | Train(CE): 4.5125 | Val: 4.7388 | AvgTrain: 4.7131 | R: 0.732
   New best: 4.7388 (Saved)â”â”â”â”â”â”â”  60% â€¢ Loss: 4.5125 2:38:15
Step  9250 | Train(CE): 4.5450 | Val: 4.6965 | AvgTrain: 4.7041 | R: 0.739
   New best: 4.6965 (Saved)â”â”â”â”â”â”â”  62% â€¢ Loss: 4.5450 2:42:39
Step  9500 | Train(CE): 4.7640 | Val: 4.7290 | AvgTrain: 4.6779 | R: 0.746
   Patience: 1/8â”â•ºâ”â”â”â”â”â”â”â”â”â”â”â”â”â”  63% â€¢ Loss: 4.7640 2:47:03
Step  9750 | Train(CE): 4.9973 | Val: 4.6738 | AvgTrain: 4.6678 | R: 0.753
   New best: 4.6738 (Saved)â”â”â”â”â”â”â”  65% â€¢ Loss: 4.9973 2:51:27
Step 10000 | Train(CE): 4.6276 | Val: 4.6664 | AvgTrain: 4.6519 | R: 0.760
   New best: 4.6664 (Saved)â”â”â”â”â”â”â”  67% â€¢ Loss: 4.6276 2:55:50
Step 10250 | Train(CE): 4.6031 | Val: 4.7226 | AvgTrain: 4.6208 | R: 0.767
   Patience: 1/8â”â”â”â•ºâ”â”â”â”â”â”â”â”â”â”â”â”  68% â€¢ Loss: 4.6031 3:00:14
Step 10500 | Train(CE): 4.6320 | Val: 4.6428 | AvgTrain: 4.6177 | R: 0.774
   New best: 4.6428 (Saved)â”â”â”â”â”â”â”  70% â€¢ Loss: 4.6320 3:04:38
Step 10750 | Train(CE): 4.5284 | Val: 4.6595 | AvgTrain: 4.6055 | R: 0.781
   Patience: 1/8â”â”â”â”â•¸â”â”â”â”â”â”â”â”â”â”â”  72% â€¢ Loss: 4.5284 3:09:02
Step 11000 | Train(CE): 4.7607 | Val: 4.6493 | AvgTrain: 4.5959 | R: 0.788
   Patience: 2/8â”â”â”â”â”â•ºâ”â”â”â”â”â”â”â”â”â”  73% â€¢ Loss: 4.7607 3:13:25
Step 11250 | Train(CE): 4.5727 | Val: 4.5917 | AvgTrain: 4.5898 | R: 0.795
   New best: 4.5917 (Saved)â”â”â”â”â”â”â”  75% â€¢ Loss: 4.5727 3:17:50
Step 11500 | Train(CE): 4.4433 | Val: 4.5989 | AvgTrain: 4.5810 | R: 0.802
   Patience: 1/8â”â”â”â”â”â”â•¸â”â”â”â”â”â”â”â”â”  77% â€¢ Loss: 4.4433 3:22:13
Step 11750 | Train(CE): 4.6619 | Val: 4.5958 | AvgTrain: 4.5638 | R: 0.809
   Patience: 2/8â”â”â”â”â”â”â”â•ºâ”â”â”â”â”â”â”â”  78% â€¢ Loss: 4.6619 3:26:38
Step 12000 | Train(CE): 4.4939 | Val: 4.5895 | AvgTrain: 4.5655 | R: 0.816
   Patience: 3/8â”â”â”â”â”â”â”â”â•ºâ”â”â”â”â”â”â”  80% â€¢ Loss: 4.4939 3:31:02
Step 12250 | Train(CE): 4.6599 | Val: 4.6202 | AvgTrain: 4.5506 | R: 0.823
   Patience: 4/8â”â”â”â”â”â”â”â”â•¸â”â”â”â”â”â”â”  82% â€¢ Loss: 4.6599 3:35:26
Step 12500 | Train(CE): 4.4023 | Val: 4.5405 | AvgTrain: 4.5415 | R: 0.830
   New best: 4.5405 (Saved)mâ”â”â”â”â”â”  83% â€¢ Loss: 4.4023 3:39:50
Step 12750 | Train(CE): 4.5243 | Val: 4.5326 | AvgTrain: 4.5325 | R: 0.837
   New best: 4.5326 (Saved)0mâ”â”â”â”â”  85% â€¢ Loss: 4.5243 3:44:14
Step 13000 | Train(CE): 4.6823 | Val: 4.5548 | AvgTrain: 4.5393 | R: 0.843
   Patience: 1/8â”â”â”â”â”â”â”â”â”â”â•¸â”â”â”â”â”  87% â€¢ Loss: 4.6823 3:48:38
Step 13250 | Train(CE): 4.4647 | Val: 4.5358 | AvgTrain: 4.5263 | R: 0.850
   Patience: 2/8â”â”â”â”â”â”â”â”â”â”â”â•ºâ”â”â”â”  88% â€¢ Loss: 4.4647 3:53:01
Step 13500 | Train(CE): 4.6001 | Val: 4.5371 | AvgTrain: 4.5244 | R: 0.857
   Patience: 3/8â”â”â”â”â”â”â”â”â”â”â”â”â•ºâ”â”â”  90% â€¢ Loss: 4.6001 3:57:25
Step 13750 | Train(CE): 4.2539 | Val: 4.5454 | AvgTrain: 4.5215 | R: 0.864
   Patience: 4/8â”â”â”â”â”â”â”â”â”â”â”â”â•¸â”â”â”  92% â€¢ Loss: 4.2539 4:01:49
Step 14000 | Train(CE): 4.4425 | Val: 4.5399 | AvgTrain: 4.5230 | R: 0.870
   Patience: 5/8â”â”â”â”â”â”â”â”â”â”â”â”â”â•ºâ”â”  93% â€¢ Loss: 4.4425 4:06:13
Step 14250 | Train(CE): 4.5401 | Val: 4.4781 | AvgTrain: 4.5239 | R: 0.877
   New best: 4.4781 (Saved)mâ”  95% â€¢ Loss: 4.5401 4:10:37
Step 14500 | Train(CE): 4.5580 | Val: 4.5747 | AvgTrain: 4.5351 | R: 0.884
   Patience: 1/8â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸â”  97% â€¢ Loss: 4.5580 4:15:01
Step 14750 | Train(CE): 4.2933 | Val: 4.5732 | AvgTrain: 4.5329 | R: 0.891
   Patience: 2/8â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•º  98% â€¢ Loss: 4.2933 4:19:25
Step 15000 | Train(CE): 4.4263 | Val: 4.5769 | AvgTrain: 4.5428 | R: 0.897
   Patience: 3/8â”â”â”â”â”â”â” 100% â€¢ Loss: 4.4263 4:23:46
  Training â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% â€¢ Loss: 4.4263 4:23:46
â™»ï¸  Restoring best model (Val: 4.4781)

ğŸ¨ Generating sample (Visual Check)...
ğŸ“ Generation Check:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Full Physics (RGD + QFE) Output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ The theory of quantum mechanics suggests that all of the laws in the theory  â”‚
â”‚ of relativity of relativity is the justification of relativityâ€™s inertia.    â”‚
â”‚ This theory of fantasy is that it has a hypothesis that physicists have      â”‚
â”‚ argued that the quantum mechanics of quantum mechanics is not yet true, but  â”‚
â”‚ exactly are produced a means of electromaserists.                            â”‚
â”‚ It is very clearly that it is not a matter of matter. It is a quantum method â”‚
â”‚ of relativity which is made possible for what is being connected. In         â”‚
â”‚ reality, the theory of theory is no longer                                   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

============================================================
ğŸ“Š Parameters: 67,473,706 (67.47M)
ğŸ”€ Using DataParallel on 2 GPUs
          ğŸ“š Dataset Statistics           
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Metric           â”ƒ Value               â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ Total Tokens     â”‚ 500,000,000         â”‚
â”‚ Train Split      â”‚ 450,000,000 (90.0%) â”‚
â”‚ Validation Split â”‚ 50,000,000 (10.0%)  â”‚
â”‚ Tokenizer        â”‚ TikToken (GPT-2)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          ğŸ§  Model Parameter Breakdown          
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Component          â”ƒ Parameters â”ƒ % of Total â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ Embeddings         â”‚ 33,921,361 â”‚      50.3% â”‚
â”‚ Attention          â”‚  4,737,032 â”‚       7.0% â”‚
â”‚ MLP                â”‚  9,452,544 â”‚      14.0% â”‚
â”‚ Other (Norms/Head) â”‚ 19,362,769 â”‚      28.7% â”‚
â”‚ TOTAL              â”‚ 67,473,706 â”‚     100.0% â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
ğŸ“Š Ratio: 7.41 Tokens per Parameter (Target > 7.4)
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ RGD Only                                                                     â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
ğŸ“Š Parameters: 67,473,706 (67.47M)
ğŸ”„ Steps: 15000
âš¡ Optimizer: RGD (strength=0.3)
ğŸ“‰ Loss: Cross-Entropy
ğŸ“ˆ LR: 1e-03
Step   250 | Train(CE): 6.8496 | Val: 6.8623 | AvgTrain: 7.9803 | R: 0.505
   New best: 6.8623 (Saved)0m   2% â€¢ Loss: 6.8496 0:03:39
Step   500 | Train(CE): 6.4259 | Val: 6.4338 | AvgTrain: 6.5851 | R: 0.494
   New best: 6.4338 (Saved)â”â”â”â”â”â”â”   3% â€¢ Loss: 6.4259 0:07:20
Step   750 | Train(CE): 6.2227 | Val: 6.2217 | AvgTrain: 6.3113 | R: 0.497
   New best: 6.2217 (Saved)â”â”â”â”â”â”â”   5% â€¢ Loss: 6.2227 0:11:00
Step  1000 | Train(CE): 6.1942 | Val: 6.0397 | AvgTrain: 6.1119 | R: 0.504
   New best: 6.0397 (Saved)â”â”â”â”â”â”â”   7% â€¢ Loss: 6.1942 0:14:41
Step  1250 | Train(CE): 5.7886 | Val: 5.9547 | AvgTrain: 5.9954 | R: 0.511
   New best: 5.9547 (Saved)â”â”â”â”â”â”â”   8% â€¢ Loss: 5.7886 0:18:22
Step  1500 | Train(CE): 5.6473 | Val: 5.8740 | AvgTrain: 5.9156 | R: 0.517
   New best: 5.8740 (Saved)â”â”â”â”â”â”â”  10% â€¢ Loss: 5.6473 0:22:03
Step  1750 | Train(CE): 5.8612 | Val: 5.7852 | AvgTrain: 5.8245 | R: 0.524
   New best: 5.7852 (Saved)â”â”â”â”â”â”â”  12% â€¢ Loss: 5.8612 0:25:43
Step  2000 | Train(CE): 5.6286 | Val: 5.7726 | AvgTrain: 5.7629 | R: 0.531
   New best: 5.7726 (Saved)â”â”â”â”â”â”â”  13% â€¢ Loss: 5.6286 0:29:24
Step  2250 | Train(CE): 5.8851 | Val: 5.7228 | AvgTrain: 5.7141 | R: 0.537
   New best: 5.7228 (Saved)â”â”â”â”â”â”â”  15% â€¢ Loss: 5.8851 0:33:05
Step  2500 | Train(CE): 5.6630 | Val: 5.5883 | AvgTrain: 5.6521 | R: 0.544
   New best: 5.5883 (Saved)â”â”â”â”â”â”â”  17% â€¢ Loss: 5.6630 0:36:46
Step  2750 | Train(CE): 5.5796 | Val: 5.6281 | AvgTrain: 5.6257 | R: 0.551
   Patience: 1/80mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  18% â€¢ Loss: 5.5796 0:40:26
Step  3000 | Train(CE): 5.6454 | Val: 5.5617 | AvgTrain: 5.5711 | R: 0.557
   New best: 5.5617 (Saved)â”â”â”â”â”â”â”  20% â€¢ Loss: 5.6454 0:44:07
Step  3250 | Train(CE): 5.4837 | Val: 5.5299 | AvgTrain: 5.5458 | R: 0.564
   New best: 5.5299 (Saved)â”â”â”â”â”â”â”  22% â€¢ Loss: 5.4837 0:47:47
Step  3500 | Train(CE): 5.7045 | Val: 5.4836 | AvgTrain: 5.5024 | R: 0.572
   New best: 5.4836 (Saved)â”â”â”â”â”â”â”  23% â€¢ Loss: 5.7045 0:51:28
Step  3750 | Train(CE): 5.6096 | Val: 5.4734 | AvgTrain: 5.4659 | R: 0.579
   New best: 5.4734 (Saved)â”â”â”â”â”â”â”  25% â€¢ Loss: 5.6096 0:55:09
Step  4000 | Train(CE): 5.4676 | Val: 5.4027 | AvgTrain: 5.4345 | R: 0.586
   New best: 5.4027 (Saved)â”â”â”â”â”â”â”  27% â€¢ Loss: 5.4676 0:58:50
Step  4250 | Train(CE): 5.3214 | Val: 5.4053 | AvgTrain: 5.4154 | R: 0.593
   Patience: 1/8mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  28% â€¢ Loss: 5.3214 1:02:30
Step  4500 | Train(CE): 5.3572 | Val: 5.3941 | AvgTrain: 5.3727 | R: 0.600
   New best: 5.3941 (Saved)â”â”â”â”â”â”â”  30% â€¢ Loss: 5.3572 1:06:11
Step  4750 | Train(CE): 5.1694 | Val: 5.3222 | AvgTrain: 5.3548 | R: 0.607
   New best: 5.3222 (Saved)â”â”â”â”â”â”â”  32% â€¢ Loss: 5.1694 1:09:51
Step  5000 | Train(CE): 5.3128 | Val: 5.2745 | AvgTrain: 5.3174 | R: 0.615
   New best: 5.2745 (Saved)â”â”â”â”â”â”â”  33% â€¢ Loss: 5.3128 1:13:32
Step  5250 | Train(CE): 5.2931 | Val: 5.2511 | AvgTrain: 5.2846 | R: 0.622
   New best: 5.2511 (Saved)â”â”â”â”â”â”â”  35% â€¢ Loss: 5.2931 1:17:12
Step  5500 | Train(CE): 5.0049 | Val: 5.2181 | AvgTrain: 5.2117 | R: 0.630
   New best: 5.2181 (Saved)â”â”â”â”â”â”â”  37% â€¢ Loss: 5.0049 1:20:53
Step  5750 | Train(CE): 5.1764 | Val: 5.1020 | AvgTrain: 5.1598 | R: 0.637
   New best: 5.1020 (Saved)â”â”â”â”â”â”â”  38% â€¢ Loss: 5.1764 1:24:34
Step  6000 | Train(CE): 4.8786 | Val: 5.0724 | AvgTrain: 5.1357 | R: 0.644
   New best: 5.0724 (Saved)â”â”â”â”â”â”â”  40% â€¢ Loss: 4.8786 1:28:13
Step  6250 | Train(CE): 5.0495 | Val: 5.0073 | AvgTrain: 5.0834 | R: 0.651
   New best: 5.0073 (Saved)â”â”â”â”â”â”â”  42% â€¢ Loss: 5.0495 1:31:53
Step  6500 | Train(CE): 4.9667 | Val: 5.0320 | AvgTrain: 5.0454 | R: 0.659
   Patience: 1/80mâ•ºâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  43% â€¢ Loss: 4.9667 1:35:33
Step  6750 | Train(CE): 4.8566 | Val: 4.9805 | AvgTrain: 5.0119 | R: 0.666
   New best: 4.9805 (Saved)â”â”â”â”â”â”â”  45% â€¢ Loss: 4.8566 1:39:13
Step  7000 | Train(CE): 5.0398 | Val: 4.9384 | AvgTrain: 4.9700 | R: 0.673
   New best: 4.9384 (Saved)â”â”â”â”â”â”â”  47% â€¢ Loss: 5.0398 1:42:54
Step  7250 | Train(CE): 5.0218 | Val: 4.9402 | AvgTrain: 4.9512 | R: 0.680
   Patience: 1/8[90mâ•ºâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  48% â€¢ Loss: 5.0218 1:46:33
Step  7500 | Train(CE): 4.8787 | Val: 4.8669 | AvgTrain: 4.9285 | R: 0.688
   New best: 4.8669 (Saved)â”â”â”â”â”â”â”  50% â€¢ Loss: 4.8787 1:50:14
Step  7750 | Train(CE): 4.8915 | Val: 4.8753 | AvgTrain: 4.9057 | R: 0.695
   Patience: 1/8â•¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  52% â€¢ Loss: 4.8915 1:53:54
Step  8000 | Train(CE): 4.8853 | Val: 4.9085 | AvgTrain: 4.8827 | R: 0.702
   Patience: 2/8mâ•ºâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  53% â€¢ Loss: 4.8853 1:57:34
Step  8250 | Train(CE): 4.8249 | Val: 4.8377 | AvgTrain: 4.8567 | R: 0.710
   New best: 4.8377 (Saved)â”â”â”â”â”â”â”  55% â€¢ Loss: 4.8249 2:01:14
Step  8500 | Train(CE): 4.6766 | Val: 4.7965 | AvgTrain: 4.8431 | R: 0.717
   New best: 4.7965 (Saved)â”â”â”â”â”â”â”  57% â€¢ Loss: 4.6766 2:04:54
Step  8750 | Train(CE): 4.8681 | Val: 4.7749 | AvgTrain: 4.8283 | R: 0.724
   New best: 4.7749 (Saved)â”â”â”â”â”â”â”  58% â€¢ Loss: 4.8681 2:08:34
Step  9000 | Train(CE): 5.0143 | Val: 4.7676 | AvgTrain: 4.8032 | R: 0.732
   New best: 4.7676 (Saved)â”â”â”â”â”â”â”  60% â€¢ Loss: 5.0143 2:12:15
Step  9250 | Train(CE): 4.8054 | Val: 4.7658 | AvgTrain: 4.7830 | R: 0.739
   Patience: 1/8â•¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  62% â€¢ Loss: 4.8054 2:15:55
Step  9500 | Train(CE): 4.7374 | Val: 4.7208 | AvgTrain: 4.7778 | R: 0.746
   New best: 4.7208 (Saved)â”â”â”â”â”â”â”  63% â€¢ Loss: 4.7374 2:19:35
Step  9750 | Train(CE): 4.6892 | Val: 4.7460 | AvgTrain: 4.7631 | R: 0.753
   Patience: 1/8â”â”â•ºâ”â”â”â”â”â”â”â”â”â”â”â”â”  65% â€¢ Loss: 4.6892 2:23:15
Step 10000 | Train(CE): 4.5926 | Val: 4.7267 | AvgTrain: 4.7389 | R: 0.760
   Patience: 2/8â”â”â•¸â”â”â”â”â”â”â”â”â”â”â”â”â”  67% â€¢ Loss: 4.5926 2:26:55
Step 10250 | Train(CE): 4.6806 | Val: 4.6782 | AvgTrain: 4.7227 | R: 0.767
   New best: 4.6782 (Saved)â”â”â”â”â”â”â”  68% â€¢ Loss: 4.6806 2:30:36
Step 10500 | Train(CE): 4.5972 | Val: 4.6684 | AvgTrain: 4.7131 | R: 0.774
   New best: 4.6684 (Saved)â”â”â”â”â”â”â”  70% â€¢ Loss: 4.5972 2:34:16
Step 10750 | Train(CE): 4.8506 | Val: 4.6430 | AvgTrain: 4.6935 | R: 0.781
   New best: 4.6430 (Saved)â”â”â”â”â”â”â”  72% â€¢ Loss: 4.8506 2:37:56
Step 11000 | Train(CE): 4.7558 | Val: 4.6685 | AvgTrain: 4.7150 | R: 0.788
   Patience: 1/8â”â”â”â”â”â•ºâ”â”â”â”â”â”â”â”â”â”  73% â€¢ Loss: 4.7558 2:41:36
Step 11250 | Train(CE): 4.7364 | Val: 4.6677 | AvgTrain: 4.6754 | R: 0.795
   Patience: 2/8â”â”â”â”â”â”â•ºâ”â”â”â”â”â”â”â”â”  75% â€¢ Loss: 4.7364 2:45:16
Step 11500 | Train(CE): 4.5943 | Val: 4.6441 | AvgTrain: 4.6665 | R: 0.802
   Patience: 3/8â”â”â”â”â”â”â•¸â”â”â”â”â”â”â”â”â”  77% â€¢ Loss: 4.5943 2:48:56
Step 11750 | Train(CE): 4.4755 | Val: 4.6443 | AvgTrain: 4.6599 | R: 0.809
   Patience: 4/8â”â”â”â”â”â”â”â•ºâ”â”â”â”â”â”â”â”  78% â€¢ Loss: 4.4755 2:52:37
Step 12000 | Train(CE): 4.6936 | Val: 4.6280 | AvgTrain: 4.6610 | R: 0.816
   New best: 4.6280 (Saved)â”â”â”â”â”â”â”  80% â€¢ Loss: 4.6936 2:56:17
Step 12250 | Train(CE): 4.6579 | Val: 4.6495 | AvgTrain: 4.6463 | R: 0.823
   Patience: 1/8â”â”â”â”â”â”â”â”â•¸â”â”â”â”â”â”â”  82% â€¢ Loss: 4.6579 2:59:57
Step 12500 | Train(CE): 4.8377 | Val: 4.6125 | AvgTrain: 4.6585 | R: 0.830
   New best: 4.6125 (Saved)mâ”â”â”â”â”â”  83% â€¢ Loss: 4.8377 3:03:37
Step 12750 | Train(CE): 4.8328 | Val: 4.6081 | AvgTrain: 4.6429 | R: 0.837
   Patience: 1/8â”â”â”â”â”â”â”â”â”â”â•ºâ”â”â”â”â”  85% â€¢ Loss: 4.8328 3:07:17
Step 13000 | Train(CE): 4.6716 | Val: 4.6326 | AvgTrain: 4.6552 | R: 0.843
   Patience: 2/8â”â”â”â”â”â”â”â”â”â”â•¸â”â”â”â”â”  87% â€¢ Loss: 4.6716 3:10:57
Step 13250 | Train(CE): 4.5567 | Val: 4.5856 | AvgTrain: 4.6448 | R: 0.850
   New best: 4.5856 (Saved)90mâ”â”â”â”  88% â€¢ Loss: 4.5567 3:14:37
Step 13500 | Train(CE): 4.5649 | Val: 4.6467 | AvgTrain: 4.6309 | R: 0.857
   Patience: 1/8â”â”â”â”â”â”â”â”â”â”â”â”â•ºâ”â”â”  90% â€¢ Loss: 4.5649 3:18:17
Step 13750 | Train(CE): 4.6528 | Val: 4.5668 | AvgTrain: 4.6308 | R: 0.864
   New best: 4.5668 (Saved)[90mâ”â”â”  92% â€¢ Loss: 4.6528 3:21:57
Step 14000 | Train(CE): 4.5695 | Val: 4.6108 | AvgTrain: 4.6296 | R: 0.870
   Patience: 1/8â”â”â”â”â”â”â”â”â”â”â”â”â”â•ºâ”â”  93% â€¢ Loss: 4.5695 3:25:37
Step 14250 | Train(CE): 4.6034 | Val: 4.5993 | AvgTrain: 4.6235 | R: 0.877
   Patience: 2/8â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•ºâ”  95% â€¢ Loss: 4.6034 3:29:17
Step 14500 | Train(CE): 4.6583 | Val: 4.6600 | AvgTrain: 4.6348 | R: 0.884
   Patience: 3/8â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸â”  97% â€¢ Loss: 4.6583 3:32:57
Step 14750 | Train(CE): 4.5465 | Val: 4.6265 | AvgTrain: 4.6407 | R: 0.891
   Patience: 4/8â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•º  98% â€¢ Loss: 4.5465 3:36:37
Step 15000 | Train(CE): 4.7395 | Val: 4.6081 | AvgTrain: 4.6517 | R: 0.897
   Patience: 5/8â”â”â”â”â”â”â” 100% â€¢ Loss: 4.7395 3:40:15
  Training â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% â€¢ Loss: 4.7395 3:40:15
â™»ï¸  Restoring best model (Val: 4.5668)

ğŸ¨ Generating sample (Visual Check)...
ğŸ“ Generation Check:
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ RGD Only Output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ The theory of quantum mechanics suggests that water vapor generated by other â”‚
â”‚ energy generating energy in the clock travels down to the planetary axis,    â”‚
â”‚ creating a low-power of energy. The first solar system, known as the         â”‚
â”‚ "energy" of the possibility is to be able to estimate what is known as the   â”‚
â”‚ solar orbit. (1)                                                             â”‚
â”‚ The amount of solar energy in the solar system is straightforward; the       â”‚
â”‚ possibility that the Sun is moving from the solar system is reflected, and   â”‚
â”‚ that the Sun's orbit is using solar energy.                                  â”‚
â”‚ When the Sun                                                                 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
                           ğŸ”¬ Ablation Study Results                            
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”³â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Experiment               â”ƒ RGD â”ƒ QFE â”ƒ Val Loss â”ƒ Perplexity â”ƒ Speed (tok/s) â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â•‡â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ Full Physics (RGD + QFE) â”‚  âœ“  â”‚  âœ“  â”‚   4.5620 â”‚      95.77 â”‚         3,881 â”‚
â”‚ RGD Only                 â”‚  âœ“  â”‚  âœ—  â”‚   4.7510 â”‚     115.70 â”‚         4,648 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“ Results saved to: 
/kaggle/working/spectral_cnn/spectral_gpt/experiment_results
âœ… Experiment suite complete!