# Experiment Monitoring Infrastructure

Robust experiment monitoring system for Spectral GPT with checkpointing, metrics logging, visualization, and configuration tracking.

## Features

### âœ… Implemented

- **Experiment ID Generation**: Unique IDs with timestamp and git hash
- **Directory Structure**: Organized experiment directories with checkpoints, logs, and visualizations
- **CheckpointManager**: Robust checkpointing with atomic writes, retention policy, and resumption support
- **MetricsLogger**: Incremental JSONL logging with immediate persistence
- **VisualizationManager**: Automated plot generation for training dynamics and model internals
- **ConfigTracker**: Configuration and results tracking with hardware info

### ðŸš§ Coming Soon

- PaperGenerator: Automated documentation generation
- ResultsAggregator: Cross-experiment comparison and analysis

## Quick Start

### Basic Usage

```python
from monitoring import (
    generate_experiment_id,
    create_experiment_directory,
    CheckpointManager,
    MetricsLogger,
    VisualizationManager,
    ConfigTracker
)

# 1. Create experiment
exp_id = generate_experiment_id("my_experiment")
dirs = create_experiment_directory("experiments", exp_id)

# 2. Initialize all monitoring components
checkpoint_manager = CheckpointManager(
    experiment_dir=dirs['root'],
    save_interval=1000,
    keep_last_n=3
)

metrics_logger = MetricsLogger(
    log_dir=dirs['logs'],
    log_interval=10
)

viz_manager = VisualizationManager(
    viz_dir=dirs['visualizations'],
    viz_interval=1000
)

config_tracker = ConfigTracker(
    experiment_dir=dirs['root']
)

# 3. Save initial configuration
config = {'lr': 0.001, 'batch_size': 32}
config_tracker.save_config(config, model=model)

# 4. Training loop with full monitoring
loss_history = []
metrics_history = {'learning_rate': [], 'perplexity': []}

for step in range(1, max_steps + 1):
    # Training step
    loss = train_step(model, optimizer, data)
    loss_history.append(loss)
    
    # Log metrics
    if metrics_logger.should_log(step):
        metrics = {
            'loss': loss,
            'learning_rate': optimizer.param_groups[0]['lr'],
            'perplexity': compute_perplexity(loss)
        }
        metrics_logger.log_metrics(step, metrics)
        
        # Track for visualization
        for key, value in metrics.items():
            if key in metrics_history:
                metrics_history[key].append(value)
    
    # Save checkpoint
    if checkpoint_manager.should_checkpoint(step):
        checkpoint_manager.save_checkpoint(
            step, model, optimizer, loss_history, config
        )
    
    # Generate visualizations
    if viz_manager.should_visualize(step):
        viz_manager.generate_training_plots(step, loss_history, metrics_history)
        viz_manager.generate_model_plots(step, model)

# 5. Save final results
final_metrics = {
    'val_loss': loss_history[-1],
    'best_val_loss': min(loss_history)
}
config_tracker.save_results(final_metrics)

# 6. Resume from interruption
checkpoint = checkpoint_manager.load_latest_checkpoint()
if checkpoint:
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    start_step = checkpoint['step'] + 1
```

## CheckpointManager API

### Initialization

```python
manager = CheckpointManager(
    experiment_dir: str,      # Root directory for experiment
    save_interval: int = 1000, # Steps between checkpoints
    keep_last_n: int = 3      # Number of checkpoints to retain
)
```

### Methods

#### `should_checkpoint(step: int) -> bool`
Check if current step requires checkpointing.

#### `save_checkpoint(step, model, optimizer, loss_history, config) -> str`
Save checkpoint with atomic write. Returns path to checkpoint file.

**Checkpoint Contents:**
- `step`: Current training step
- `model_state_dict`: Model parameters
- `optimizer_state_dict`: Optimizer state
- `loss_history`: List of training losses
- `config`: Experiment configuration
- `timestamp`: ISO format timestamp
- `git_hash`: Git commit hash

#### `load_latest_checkpoint() -> Optional[Dict]`
Load most recent checkpoint. Returns None if no checkpoint exists.

#### `load_checkpoint(step: int) -> Optional[Dict]`
Load specific checkpoint by step number.

#### `list_checkpoints() -> List[Dict]`
List all available checkpoints with metadata.

#### `cleanup_old_checkpoints()`
Remove checkpoints beyond retention limit (called automatically).

## Directory Structure

```
experiments/
â””â”€â”€ {experiment_id}/
    â”œâ”€â”€ checkpoints/
    â”‚   â”œâ”€â”€ checkpoint_step_1000.pt
    â”‚   â”œâ”€â”€ checkpoint_step_2000.pt
    â”‚   â”œâ”€â”€ checkpoint_step_3000.pt
    â”‚   â””â”€â”€ checkpoint_latest.pt -> checkpoint_step_3000.pt
    â”œâ”€â”€ logs/
    â”‚   â”œâ”€â”€ metrics.jsonl (coming soon)
    â”‚   â””â”€â”€ training.log (coming soon)
    â”œâ”€â”€ visualizations/ (coming soon)
    â”œâ”€â”€ config.json (coming soon)
    â””â”€â”€ results.json (coming soon)
```

## Experiment ID Format

Format: `{name}_{timestamp}_{git_hash}`

Example: `wave_rgd_qfe_20241209_143022_a3f2b1c4`

- **name**: Experiment name
- **timestamp**: `YYYYMMDD_HHMMSS` format
- **git_hash**: Short git commit hash (8 chars)

## Key Features

### Atomic Writes
Checkpoints use temporary files and atomic rename to prevent corruption:
```python
# Write to temp file
torch.save(checkpoint_data, temp_path)
# Atomic rename (POSIX guarantee)
temp_path.replace(checkpoint_path)
```

### Automatic Cleanup
Old checkpoints are automatically removed based on retention policy:
- Keeps only N most recent checkpoints
- Sorted by step number
- Prevents disk space issues

### Symlink to Latest
`checkpoint_latest.pt` always points to most recent checkpoint for easy resumption.

### DataParallel Support
Automatically handles `nn.DataParallel` wrapped models:
```python
if isinstance(model, nn.DataParallel):
    model_state = model.module.state_dict()
```

## Testing

Run the test suite:
```bash
python tests/test_monitoring.py
```

Run the demo:
```bash
python spectral_gpt/demo_monitoring.py
```

## Requirements

- Python 3.7+
- PyTorch 1.0+
- Git (for commit hash tracking)

## Implementation Details

### Checkpoint Format

Checkpoints are saved as PyTorch `.pt` files containing:

```python
{
    'step': int,
    'model_state_dict': OrderedDict,
    'optimizer_state_dict': Dict,
    'loss_history': List[float],
    'config': Dict,
    'timestamp': str,  # ISO format
    'git_hash': str    # Short hash
}
```

### Error Handling

- **Corrupted checkpoint**: Falls back to previous checkpoint
- **Missing symlink**: Searches for most recent checkpoint by step
- **Disk full**: Gracefully skips checkpoint (future enhancement)
- **Permission errors**: Clear error messages

## Examples

### Example 1: Basic Training Loop

```python
manager = CheckpointManager("experiments/exp_001", save_interval=500)

for step in range(1, 10001):
    loss = train_step(model, optimizer, data)
    losses.append(loss)
    
    if manager.should_checkpoint(step):
        manager.save_checkpoint(step, model, optimizer, losses, config)
        print(f"Checkpoint saved at step {step}")
```

### Example 2: Resume from Interruption

```python
manager = CheckpointManager("experiments/exp_001")

# Try to load checkpoint
checkpoint = manager.load_latest_checkpoint()

if checkpoint:
    print(f"Resuming from step {checkpoint['step']}")
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    losses = checkpoint['loss_history']
    start_step = checkpoint['step'] + 1
else:
    print("Starting from scratch")
    start_step = 1
    losses = []

# Continue training
for step in range(start_step, max_steps + 1):
    # ... training ...
```

### Example 3: List and Inspect Checkpoints

```python
manager = CheckpointManager("experiments/exp_001")

# List all checkpoints
checkpoints = manager.list_checkpoints()
for cp in checkpoints:
    print(f"Step {cp['step']}: {cp['size_mb']:.2f} MB")

# Load specific checkpoint
checkpoint = manager.load_checkpoint(step=5000)
if checkpoint:
    print(f"Loss at step 5000: {checkpoint['loss_history'][-1]:.4f}")
```

## Design Decisions

1. **JSONL for logs** (coming): One JSON object per line for streaming and easy parsing
2. **Atomic writes**: Prevent corruption from interrupted saves
3. **Retention policy**: Automatic cleanup to manage disk space
4. **Symlink pattern**: Easy access to latest checkpoint
5. **Git integration**: Automatic commit hash tracking for reproducibility
6. **Minimal dependencies**: Only PyTorch and standard library

## Future Enhancements

- Distributed training support (multi-GPU coordination)
- Cloud storage integration (S3, GCS)
- Compression for large checkpoints
- Checkpoint validation and integrity checks
- Automatic backup to remote storage
- Web dashboard for monitoring

## Contributing

When adding new features:
1. Follow existing code style
2. Add tests to `tests/test_monitoring.py`
3. Update this README
4. Ensure all tests pass

## License

Same as parent project.


## MetricsLogger API

### Initialization

```python
logger = MetricsLogger(
    log_dir: str,           # Directory for log files
    log_interval: int = 10  # Steps between log entries
)
```

### Methods

#### `should_log(step: int) -> bool`
Check if current step requires logging.

#### `log_metrics(step, metrics, flush=True)`
Append metrics to log file with immediate persistence.

**Metrics Format (JSONL):**
```json
{"step": 10, "timestamp": "2024-12-09T14:30:00", "loss": 1.5, "learning_rate": 0.001}
```

#### `load_metrics() -> List[Dict]`
Load all logged metrics from file.

#### `get_latest_step() -> int`
Get the last logged step number.

#### `get_metrics_summary() -> Dict`
Get summary statistics (min, max, mean, final) for all metrics.

## VisualizationManager API

### Initialization

```python
viz_manager = VisualizationManager(
    viz_dir: str,             # Directory for visualization files
    viz_interval: int = 1000  # Steps between visualization generation
)
```

### Methods

#### `should_visualize(step: int) -> bool`
Check if current step requires visualization.

#### `generate_training_plots(step, loss_history, metrics)`
Generate plots of training dynamics:
- Loss curves
- Learning rate schedule
- Other tracked metrics (perplexity, wave_ratio, etc.)

#### `generate_model_plots(step, model)`
Generate plots of model internals (for wave models):
- Frequency distributions
- Phase distributions
- Harmonic amplitudes
- Wave packets
- Interference patterns

**Note**: Gracefully handles models without wave attributes.

#### `generate_comparison_plots(experiments)`
Generate comparison plots across multiple experiments.

### Visualization Features

- **Dark theme**: Consistent matplotlib styling
- **Timestamped filenames**: Track evolution over training
- **Error handling**: Continues training even if visualization fails
- **Multiple plot types**: Training dynamics and model internals

## ConfigTracker API

### Initialization

```python
tracker = ConfigTracker(
    experiment_dir: str  # Root directory for experiment
)
```

### Methods

#### `save_config(config, model=None, dataset_info=None)`
Save experiment configuration with:
- All hyperparameters
- Model architecture details (if model provided)
- Dataset information (if provided)
- Git commit hash
- Timestamp
- Hardware info (GPU, CPU, memory)

#### `load_config() -> Dict`
Load experiment configuration.

#### `save_results(final_metrics, best_checkpoint=None, generation_samples=None)`
Save final experiment results.

### Configuration Format

```json
{
  "experiment_id": "wave_rgd_20241209_143022_a3f2b1c4",
  "timestamp": "2024-12-09T14:30:22",
  "git_hash": "a3f2b1c4",
  "config": {
    "learning_rate": 0.001,
    "batch_size": 32
  },
  "model": {
    "type": "WaveGPT",
    "num_parameters": 1234567,
    "d_model": 512,
    "num_layers": 6
  },
  "hardware": {
    "gpu_available": true,
    "gpu_model": "NVIDIA A100",
    "cuda_version": "11.8",
    "cpu_count": 32
  }
}
```

## Complete Example

### Full Monitoring Setup

```python
from monitoring import (
    generate_experiment_id,
    create_experiment_directory,
    CheckpointManager,
    MetricsLogger,
    VisualizationManager,
    ConfigTracker
)

# Setup experiment
exp_id = generate_experiment_id("wave_gpt_experiment")
dirs = create_experiment_directory("experiments", exp_id)

# Initialize all components
checkpoint_manager = CheckpointManager(
    experiment_dir=dirs['root'],
    save_interval=1000,
    keep_last_n=3
)

metrics_logger = MetricsLogger(
    log_dir=dirs['logs'],
    log_interval=10
)

viz_manager = VisualizationManager(
    viz_dir=dirs['visualizations'],
    viz_interval=1000
)

config_tracker = ConfigTracker(
    experiment_dir=dirs['root']
)

# Save configuration
config = {
    'model': {'d_model': 512, 'num_layers': 6},
    'training': {'lr': 0.001, 'batch_size': 32}
}
config_tracker.save_config(config, model=model)

# Training loop
loss_history = []
metrics_history = {'learning_rate': [], 'perplexity': []}

for step in range(1, max_steps + 1):
    # Training
    loss = train_step(model, optimizer, data)
    loss_history.append(loss)
    
    # Metrics logging
    if metrics_logger.should_log(step):
        metrics = {
            'loss': loss,
            'learning_rate': optimizer.param_groups[0]['lr'],
            'perplexity': compute_perplexity(loss)
        }
        metrics_logger.log_metrics(step, metrics)
        
        for key, value in metrics.items():
            if key in metrics_history:
                metrics_history[key].append(value)
    
    # Checkpointing
    if checkpoint_manager.should_checkpoint(step):
        checkpoint_manager.save_checkpoint(
            step, model, optimizer, loss_history, config
        )
    
    # Visualization
    if viz_manager.should_visualize(step):
        viz_manager.generate_training_plots(step, loss_history, metrics_history)
        viz_manager.generate_model_plots(step, model)

# Save results
final_metrics = {
    'val_loss': loss_history[-1],
    'best_val_loss': min(loss_history),
    'best_step': loss_history.index(min(loss_history)) + 1
}
config_tracker.save_results(final_metrics)
```

## Visualization Examples

### Training Dynamics Plot
Shows loss curves, learning rate schedule, and other metrics over time.

### Model Internals Plot (Wave Models)
For wave-based models, shows:
- **Frequency distributions**: Heatmap and histogram of base frequencies
- **Phase distributions**: Heatmap and polar plot of phases
- **Harmonic amplitudes**: Mean harmonic profile and distribution
- **Wave packets**: Superposition of waves for sample tokens

### Comparison Plot
Compares multiple experiments side-by-side:
- Loss curves
- Learning rate schedules
- Perplexity
- Final metrics bar chart

## Best Practices

1. **Set appropriate intervals**:
   - Checkpoints: Every 1000-5000 steps (balance safety vs disk space)
   - Metrics: Every 10-100 steps (balance detail vs file size)
   - Visualizations: Every 1000-5000 steps (balance insight vs time)

2. **Monitor disk space**:
   - Checkpoints can be large (100s of MB)
   - Use `keep_last_n` to limit checkpoint count
   - Visualizations are typically small (100s of KB)

3. **Use consistent naming**:
   - Include experiment type in name: `wave_rgd_qfe`
   - Git hash ensures reproducibility

4. **Save configuration early**:
   - Call `config_tracker.save_config()` before training
   - Ensures config is saved even if training crashes

5. **Check for existing checkpoints**:
   - Always check `load_latest_checkpoint()` before training
   - Allows seamless resumption

## Troubleshooting

### Checkpoint not loading
- Check file permissions
- Verify checkpoint file exists and is not corrupted
- Try loading specific checkpoint with `load_checkpoint(step)`

### Metrics not persisting
- Ensure `flush=True` in `log_metrics()` (default)
- Check disk space
- Verify write permissions

### Visualizations not generating
- Check matplotlib installation
- Verify model has required attributes (for model plots)
- Check error messages (visualization failures don't stop training)

### Out of disk space
- Reduce `keep_last_n` for checkpoints
- Increase checkpoint interval
- Clean up old experiments

## Performance Considerations

- **Checkpointing**: ~1-2 seconds for large models (doesn't block training)
- **Metrics logging**: <1ms per log entry (immediate flush)
- **Visualization**: ~1-5 seconds per plot set (can run in background)

Total overhead: <1% of training time with recommended intervals.
