# ğŸŒŠ Wave-Native GPT

> "Everything in physics is a mass on a spring" â€” Wave-based language modeling

## Philosophy

Standard transformers fight against wave-based computation by using discrete token embeddings. **Wave-Native GPT** makes language itself continuous:

```
Token â†’ Wave Packet â†’ Interference â†’ Superposition â†’ Collapse â†’ Next Token
```

## Architecture

### 1. Wave Packet Embedding
Tokens are embedded as wave packets, not vectors:
- **Frequency**: What "pitch" does this token resonate at?
- **Phase**: Where in the wave cycle does this token start?
- **Amplitude**: How strong is each wave component?

### 2. Wave Interference Attention
Attention via constructive/destructive interference:
- Waves in phase â†’ amplify (high attention)
- Waves out of phase â†’ cancel (low attention)

### 3. Wave Collapse Head
Like quantum measurement: continuous wave state "collapses" to discrete token probabilities.

## Files

| File | Description |
|------|-------------|
| `wave_gpt.py` | Wave-Native GPT model architecture |
| `wave_benchmark.py` | Benchmark + visualization suite |
| `benchmark_results/` | Training outputs and plots |
| `prototyping/` | Legacy spectral transformer experiments |

## Quick Start

```bash
# Run benchmark on Colab (GPU recommended)
python spectral_gpt/wave_benchmark.py
```

## Benchmark Results (5M params, 5000 steps)

| Model | Perplexity | Speed | Gap |
|-------|------------|-------|-----|
| Classic Transformer | ~25 | 94K tok/s | baseline |
| **Wave-Native GPT** ğŸŒŠ | ~63 | 76K tok/s | 2.5x PPL |

Wave-Native achieves **81% of Classic's speed** while learning continuous representations!

## Visualizations

The benchmark saves interpretability plots to `benchmark_results/wave_gpt_plots/`:

- ğŸ“ˆ Learning curves (raw + smoothed)
- ğŸµ Token frequency distributions
- ğŸŒ€ Token phase heatmaps
- ğŸŒŠ Wave packet visualizations per token
- ğŸ¯ Attention phase shifts
- âš”ï¸ Comparison plots

## Key Innovations

| Component | Standard GPT | Wave-Native GPT |
|-----------|--------------|-----------------|
| Embedding | Lookup table | Wave packets |
| Representation | d_model vector | (freq, phase, amp) |
| Attention | Dot product | Wave interference |
| Activation | GELU/ReLU | sin(x) + 0.1x |
| Output | Linear | Wave collapse |

## Future Directions

1. **Holographic memory**: Full wave interference as associative memory
2. **Diffusion + Waves**: Denoising in wave space
3. **Complex-valued**: Use â„‚ instead of â„ for true wave computation
4. **Resonance learning**: Let tokens "resonate" with each other

---

*Part of the Spectral Neural Networks research project*
